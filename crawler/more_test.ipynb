{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used for update in 2022/5/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle\n",
    "import time\n",
    "import couchdb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### definitions\n",
    "def reform_tweet(raw_tweet):\n",
    "    \"\"\"\n",
    "    Reform the tweet data into a dictionary form.\n",
    "    Each tweet is a dictionary contains: tweet_id, text, author_id, create_at, geo\n",
    "    \"\"\"\n",
    "    tweet_dict = dict()\n",
    "    tweet_dict['_id'] = str(raw_tweet.id)\n",
    "    tweet_dict['author_id'] = raw_tweet.author_id\n",
    "    tweet_dict['text'] = raw_tweet.text\n",
    "    tweet_dict['created_at'] = str(raw_tweet.created_at)\n",
    "    tweet_dict['geo'] = raw_tweet.geo\n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "\n",
    "def preprocess(raw_tweet, tweet_list):\n",
    "    \"\"\"\n",
    "    Process all tweets in one turn, remove tweets withou geo info\n",
    "    \"\"\"\n",
    "    for data in raw_tweet:\n",
    "        tweet_list.append(reform_tweet(data))\n",
    "    \n",
    "    return tweet_list\n",
    "\n",
    "\n",
    "def get_tweet_1(query, token, tweets_each_turn, next_page = None):\n",
    "    \"\"\"\n",
    "    Get tweet using Twitter API 2.0. \n",
    "    Using keywords(query) as a main parameter\n",
    "    Caputure tweet_id, text, geo, author_id, create_at\n",
    "    \"\"\"\n",
    "    client = tweepy.Client(bearer_token= token)\n",
    "    # each time capture 100 tweets\n",
    "    tweets = client.search_recent_tweets(query=query, tweet_fields=['author_id', 'created_at', 'geo'], max_results=tweets_each_turn, next_token=next_page)\n",
    "\n",
    "    print(tweets.meta)\n",
    "\n",
    "    return tweets.data, tweets.meta['next_token']\n",
    "\n",
    "\n",
    "def crawler(query, tokens, tweets_each_turn, turns, client, db_name, db_name2):\n",
    "    \"\"\"\n",
    "    Main function. Use query as filter, token for authocation, and number of tweets each turn\n",
    "    Turns suggest how many turns this function will run\n",
    "    Get tweets from Twitter\n",
    "    Refrom tweets to dictionary type\n",
    "    Save tweets to CouchDB\n",
    "    Automatically repeat this process.\n",
    "    For one token, speed limit is 900 tweets/15min(1/sec)\n",
    "    \"\"\"\n",
    "\n",
    "    time_gap = int (tweets_each_turn / len(tokens)) + 1\n",
    "    \n",
    "    count = 0\n",
    "    next_page = None\n",
    "    tweet_list = []\n",
    "    while(count < turns):\n",
    "        for token in tokens:\n",
    "            data, next_page = get_tweet_1(query, token, tweets_each_turn, next_page)\n",
    "            tweet_list = preprocess(data, tweet_list)\n",
    "            time.sleep(time_gap)\n",
    "        \n",
    "        count = count + 1\n",
    "\n",
    "    #save_to_couchDB(client, tweet_list, db_name, db_name2)\n",
    "\n",
    "    output = open('tweetdata.pkl','wb')\n",
    "    pickle.dump(tweet_list, output)\n",
    "    output.close()\n",
    "    \n",
    "    return next_page\n",
    "\n",
    "\n",
    "\n",
    "def save_to_couchDB(client, tweet_data, db_name, db_name2):\n",
    "    \"\"\"\n",
    "    Save tweets to CouchDB, remove all duplicates\n",
    "    Save tweets without geo info to db_name\n",
    "    Save tweets with geo info to db_name2\n",
    "    \"\"\"\n",
    "    if(client.up() == True):\n",
    "        print(\"Connected to CouchDB\")\n",
    "    else:\n",
    "        print(\"Unable to connect to CouchDB\")\n",
    "        return\n",
    "\n",
    "    if( db_name not in client.all_dbs()):\n",
    "        print(\"No database:\" + db_name + \", create one first\")\n",
    "        client.create(db_name)\n",
    "\n",
    "    if( db_name2 not in client.all_dbs()):\n",
    "        print(\"No database:\" + db_name2 + \", create one first\")\n",
    "        client.create(db_name2)\n",
    "    \n",
    "    db1 = client.get(db_name)\n",
    "    count1 = 0\n",
    "\n",
    "    db2 = client.get(db_name2)\n",
    "    count2 = 0\n",
    "\n",
    "    for data in tweet_data:\n",
    "        if(data['geo'] == None):\n",
    "            if(data['_id'] not in db1):\n",
    "                db1.save(data)\n",
    "                count1 += 1\n",
    "        \n",
    "        else:  #store tweets with geo to another database\n",
    "            if(data['_id'] not in db2):\n",
    "                db2.save(data)\n",
    "                count2 += 1\n",
    "    \n",
    "    print(str(count1) + \" tweets(no geo) are successfully saved to database\" + db_name)\n",
    "    print(str(count2) + \" tweets(with geo) are successfully saved to database\" + db_name2)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crawler_2(query, tokens, tweets_each_turn, turns, client, db_name):\n",
    "    \"\"\"\n",
    "    Main function. Use query as filter, token for authocation, and number of tweets each turn\n",
    "    Turns suggest how many turns this function will run\n",
    "    Get tweets from Twitter\n",
    "    Refrom tweets to dictionary type\n",
    "    Save tweets to CouchDB\n",
    "    Automatically repeat this process.\n",
    "    For one token, speed limit is 900 tweets/15min(1/sec)\n",
    "    \"\"\"\n",
    "\n",
    "    time_gap = int (tweets_each_turn / len(tokens)) + 1\n",
    "    \n",
    "    count = 0\n",
    "    next_page = None\n",
    "    tweet_list = []\n",
    "    while(count < turns):\n",
    "        for token in tokens:\n",
    "            data, next_page = get_tweet_1(query, token, tweets_each_turn, next_page)\n",
    "            tweet_list = preprocess(data, tweet_list)\n",
    "            time.sleep(time_gap)\n",
    "        \n",
    "        count = count + 1\n",
    "\n",
    "    #save_to_couchDB_2(client, tweet_list, db_name)\n",
    "    output = open('topicB.pkl','wb')\n",
    "    pickle.dump(tweet_list, output)\n",
    "    output.close()\n",
    "    \n",
    "    return next_page\n",
    "\n",
    "\n",
    "\n",
    "def save_to_couchDB_2(client, tweet_data, db_name):\n",
    "    \"\"\"\n",
    "    Save tweets to CouchDB, remove all duplicates\n",
    "    Save tweets with geo info to db_name\n",
    "    Filter is based on coorinates\n",
    "    Used for specific scenario\n",
    "    \"\"\"\n",
    "\n",
    "    geo_filter = [144.33363404800002, -38.50298801599996, 145.8784120140001, -37.17509899299995]\n",
    "\n",
    "    if(client.up() == True):\n",
    "        print(\"Connected to CouchDB\")\n",
    "    else:\n",
    "        print(\"Unable to connect to CouchDB\")\n",
    "        return\n",
    "\n",
    "    if( db_name not in client.all_dbs()):\n",
    "        print(\"No database:\" + db_name + \", create one first\")\n",
    "        client.create(db_name)\n",
    "    \n",
    "    db1 = client.get(db_name)\n",
    "    count1 = 0\n",
    "\n",
    "    for data in tweet_data:\n",
    "        if(data['geo'] != None):#only store tweets with geo to database\n",
    "            if('coordinates' in data['geo'].keys()):\n",
    "                if(data['_id'] not in db1):\n",
    "                    latitude = data['geo']['coordinates']['coordinates'][0] \n",
    "                    longitude = data['geo']['coordinates']['coordinates'][1] \n",
    "                    if( latitude >= geo_filter[0] and latitude <= geo_filter[2]):\n",
    "                        if(longitude >= geo_filter[1] and longitude <= geo_filter[3]):\n",
    "                            db1.save(data)\n",
    "                            count1 += 1\n",
    "\n",
    "    print(str(count1) + \" tweets(with geo) are successfully saved to database\" + db_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set up\n",
    "BEARER_TOKEN = [\"AAAAAAAAAAAAAAAAAAAAALWBbQEAAAAA%2FbQ0tpIE3uy14yUmYU0AiocoH6c%3DDkX3Fl2TdMFgRBCivYCSMajfqglkm8DkyylcAXkUFFceAIOBRB\"]\n",
    "\n",
    "\n",
    "query1 = '(Melbourne OR melbourne) lang:en'\n",
    "query2 = '#Melbourne lang:en'\n",
    "query3 = 'suiside OR depression OR anxiety OR dying OR hallucination lang:en'\n",
    "\n",
    "client = couchdb3.Server(\n",
    "    \"http://172.26.132.196:5984\",\n",
    "    user=\"admin\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "db_name = \"raw_tweets\"\n",
    "db_name2 = \"geo_tweets\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'newest_id': '1521049510113583105', 'oldest_id': '1521040652699586565', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jy4a4ognw9h4zp81htv3wvb1'}\n",
      "{'newest_id': '1521040419273945088', 'oldest_id': '1521021839807553538', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jxiqsz6texxkqb5b1iqf2gsd'}\n",
      "{'newest_id': '1521021839639781376', 'oldest_id': '1521007292908707840', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jwxf0orijf3rtklr7564m5bx'}\n",
      "{'newest_id': '1521006881203900416', 'oldest_id': '1520989610897461254', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywksxjx8ffe8vurhawxekmewowvx'}\n",
      "{'newest_id': '1520989465078296577', 'oldest_id': '1520965848898416640', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywksxjbhim7mno8q84bfetcphif1'}\n",
      "{'newest_id': '1520965808649740289', 'oldest_id': '1520948662267830272', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywksxiq17b5m13dw2p4xp6chsd4t'}\n"
     ]
    }
   ],
   "source": [
    "if(client.up()):\n",
    "    while(True): ## infinite loop\n",
    "        #next_tokens = crawler(query1, BEARER_TOKEN, 50, 5, client, db_name, db_name2)\n",
    "        next_tokens = crawler(query2, BEARER_TOKEN, 50, 6, client, db_name, db_name2)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = open('tweetdata.pkl','rb')\n",
    "data1 = pickle.load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'place_id': '01864a8a64df9dc4', 'coordinates': {'type': 'Point', 'coordinates': [144.9630576, -37.8136276]}}\n"
     ]
    }
   ],
   "source": [
    "for data in data1:\n",
    "    if(data['geo'] != None):\n",
    "        if('coordinates' in data['geo'].keys()):\n",
    "            print(data['geo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to CouchDB\n",
      "No database:test, create one first\n",
      "1 tweets(with geo) are successfully saved to databasetest\n"
     ]
    }
   ],
   "source": [
    "save_to_couchDB_2(client, data1, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'newest_id': '1521057747806130181', 'oldest_id': '1521057650519293952', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jyfd6euaxm5eys92oft6cigt'}\n",
      "{'newest_id': '1521057649055657985', 'oldest_id': '1521057545938481154', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jyfd674puiuk0ywyy55gs9rx'}\n",
      "{'newest_id': '1521057540397797376', 'oldest_id': '1521057458982211584', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jyfd5zha88qhh6e2tfc4yut9'}\n",
      "{'newest_id': '1521057458843697152', 'oldest_id': '1521057386571743233', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jyfd5rvzmd704p9p1ks35g1p'}\n",
      "{'newest_id': '1521057386055749637', 'oldest_id': '1521057319773253632', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jyfd5rofveduw9uf2ogmxv99'}\n",
      "{'newest_id': '1521057316099051521', 'oldest_id': '1521057225057480705', 'result_count': 50, 'next_token': 'b26v89c19zqg8o3fpywl7jyfd5k0z6otpjapxfp8e1cvx'}\n"
     ]
    }
   ],
   "source": [
    "if(client.up()):\n",
    "    while(True): ## infinite loop\n",
    "        #next_tokens = crawler(query1, BEARER_TOKEN, 50, 5, client, db_name, db_name2)\n",
    "        next_tokens = crawler_2(query3, BEARER_TOKEN, 50, 6, client, db_name)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = open('topicB.pkl','rb')\n",
    "data2 = pickle.load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data2:\n",
    "    if(data['geo'] != None):\n",
    "        if('coordinates' in data['geo'].keys()):\n",
    "            print(data['geo'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ce5bc9348202541e82489c0c7283087ccf1347e0b0a31df1bd21360ebe99e17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
