{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import couchdb3\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "def reform_tweet(raw_tweet):\n",
    "    \"\"\"\n",
    "    Reform the tweet data into a dictionary form.\n",
    "    Each tweet is a dictionary contains: tweet_id, text, author_id, create_at, geo\n",
    "    \"\"\"\n",
    "    tweet_dict = dict()\n",
    "    tweet_dict['_id'] = str(raw_tweet.id)\n",
    "    tweet_dict['author_id'] = raw_tweet.author_id\n",
    "    tweet_dict['text'] = raw_tweet.text\n",
    "    tweet_dict['created_at'] = str(raw_tweet.created_at)\n",
    "    tweet_dict['geo'] = raw_tweet.geo\n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "\n",
    "def preprocess(raw_tweet, tweet_list):\n",
    "    \"\"\"\n",
    "    Process all tweets in one turn, remove tweets withou geo info\n",
    "    \"\"\"\n",
    "    for data in raw_tweet:\n",
    "        tweet_list.append(reform_tweet(data))\n",
    "    \n",
    "    return tweet_list\n",
    "\n",
    "\n",
    "def get_tweet_1(query, token, tweets_each_turn, next_page = None):\n",
    "    \"\"\"\n",
    "    Get tweet using Twitter API 2.0. \n",
    "    Using keywords(query) as a main parameter\n",
    "    Caputure tweet_id, text, geo, author_id, create_at\n",
    "    \"\"\"\n",
    "    client = tweepy.Client(bearer_token= token)\n",
    "    # each time capture 100 tweets\n",
    "    tweets = client.search_recent_tweets(query=query, tweet_fields=['author_id', 'created_at', 'geo'], max_results=tweets_each_turn, next_token=next_page)\n",
    "\n",
    "    print(tweets.meta)\n",
    "\n",
    "    return tweets.data, tweets.meta['next_token']\n",
    "\n",
    "\n",
    "def crawler(query, tokens, tweets_each_turn, turns, client, db_name):\n",
    "    \"\"\"\n",
    "    Main function. Use query as filter, token for authocation, and number of tweets each turn\n",
    "    Turns suggest how many turns this function will run\n",
    "    Get tweets from Twitter\n",
    "    Remove tweets without geo\n",
    "    Refrom tweets to dictionary type\n",
    "    Save tweets as pickle file\n",
    "    Automatically repeat this process.\n",
    "    For one token, speed limit is 900 tweets/15min(1/sec)\n",
    "    \"\"\"\n",
    "\n",
    "    time_gap = int (tweets_each_turn / len(tokens)) + 1\n",
    "    count = 0\n",
    "    next_page = None\n",
    "    tweet_list = []\n",
    "    while(count < turns):\n",
    "        for token in tokens:\n",
    "            data, next_page = get_tweet_1(query, token, tweets_each_turn, next_page)\n",
    "            tweet_list = preprocess(data, tweet_list)\n",
    "            time.sleep(time_gap)\n",
    "        \n",
    "        count = count + 1\n",
    "\n",
    "    save_to_couchDB(client, tweet_list, db_name)\n",
    "    \n",
    "    return next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_couchDB(client, tweet_data, db_name):\n",
    "    if(client.up() == True):\n",
    "        print(\"Connected to CouchDB\")\n",
    "    else:\n",
    "        print(\"Unable to connect to CouchDB\")\n",
    "        return\n",
    "\n",
    "    if( db_name not in client.all_dbs()):\n",
    "        print(\"No database:\" + db_name + \", create one first\")\n",
    "        client.create(db_name)\n",
    "    \n",
    "    db = client.get(db_name)\n",
    "    count = 0\n",
    "\n",
    "    for data in tweet_data:\n",
    "        if(data['_id'] not in db):\n",
    "            db.save(data)\n",
    "            count += 1\n",
    "    \n",
    "    print(str(count) + \" tweets is successfully saved to CouchDB\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = [\"AAAAAAAAAAAAAAAAAAAAALWBbQEAAAAA%2FbQ0tpIE3uy14yUmYU0AiocoH6c%3DDkX3Fl2TdMFgRBCivYCSMajfqglkm8DkyylcAXkUFFceAIOBRB\",\n",
    "\"AAAAAAAAAAAAAAAAAAAAAF1YbQEAAAAAEOLr26RmQ1V0eVq1xDR%2FUioYOKY%3DAHtIcXsDHv5lnyzj8KAdzlEbVVaC85k3uvvUvYESyeK0h9knqM\"]                \n",
    "query1 = '#Melbourne lang:en'\n",
    "query2 = 'Melbourne rape lang:en'\n",
    "query3 = 'Melbourne family violence lang:en'\n",
    "\n",
    "client = couchdb3.Server(\n",
    "    \"http://172.26.132.196:5984\",\n",
    "    user=\"admin\",\n",
    "    password=\"admin\"\n",
    ")\n",
    "db_name = \"renkai_tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'newest_id': '1519948731113607169', 'oldest_id': '1519945830207131648', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpytotfz03y0ypqypvkkmdlod5mgzh'}\n",
      "{'newest_id': '1519945606919245825', 'oldest_id': '1519943613941432322', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpytotfz00wcpudqe12hgnijug4j99'}\n",
      "{'newest_id': '1519943609180692481', 'oldest_id': '1519941612016709632', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpytotfyzxv3w63bokjm3gdfs1yc1p'}\n",
      "{'newest_id': '1519941331770130435', 'oldest_id': '1519939049095680000', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpytotfyphjfp5p14picl6jdi3wlml'}\n",
      "Connected to CouchDB\n",
      "24 tweets is successfully saved to CouchDB\n"
     ]
    }
   ],
   "source": [
    "next_token = crawler(query1, BEARER_TOKEN, 10, 2, client, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = open('test.pkl','rb')\n",
    "data1 = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '1519942756415643648',\n",
       " 'author_id': 1455843063440424969,\n",
       " 'text': 'Winter rains #Melbourne #melbourneweather #iPhone13Pro https://t.co/ZeAp0h5aGO',\n",
       " 'created_at': '2022-04-29 07:32:46+00:00',\n",
       " 'geo': {'place_id': '13ecc33734165000'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to CouchDB\n",
      "0 tweets is successfully saved to CouchDB\n"
     ]
    }
   ],
   "source": [
    "save_to_couchDB(client, data1, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.get(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1519936848461983745'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.all_docs()['rows'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1519936848461983745' in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = db.get('1519924999435747328')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ce5bc9348202541e82489c0c7283087ccf1347e0b0a31df1bd21360ebe99e17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
